# Experiment: BTCUSDT 1h Forecasting & Trading (2024-10-01 to 2025-12-10) – v1

## Status – 2025-12-13

### Phase 1 Status – 2025-12-13

- Ready feeds: macro (SPY 1h, realized vol), CoinAPI market (spot/perp OHLCV + volatility/basis proxies), daily CryptoQuant fallback (forward-filled to hourly).
- Fallbacks: CryptoQuant hourly pending CQ-2025-1213; CoinAPI funding endpoints still 404 despite symbol metadata (diagnostics logged under artifacts/monitoring/).
- Derived features: realized volatility, basis spreads, and funding deltas now flow through processed parquet outputs into all NPZ bundles (42 features total).
- Next activation step once vendors unblock: rerun coinapi_exchange.py and cryptoquant_daily.py → compute_cryptoquant_resampled.py / compute_coinapi_features.py → rebuild build_training_dataset*.py to refresh NPZ artifacts.

Would you prefer that I continue prototyping additional derived features or pause until the support tickets land?


- **CryptoQuant daily fallback**: Daily metrics are ingested via `data/ingestors/cryptoquant_daily.py`, which now consumes `v1/discovery/endpoints` and only calls BTC daily series explicitly granted to the token. The full discovery payload (including skipped endpoints) is persisted in `artifacts/monitoring/cryptoquant_daily_catalog.json`, every successful pull lands under `data/raw/cryptoquant_daily/metric=<name>/...` with provenance metadata, and the hourly fallback parquet (`data/processed/cryptoquant/hourly_features.parquet`) adds forward-filled levels, daily deltas, percent changes, and 30-day z-scores for the supported catalog. Coverage diagnostics stay in `artifacts/monitoring/cryptoquant_daily_summary.json`, feature counts in `artifacts/monitoring/cryptoquant_feature_catalog.json`, and unsupported endpoints are logged as skipped so we can track the outstanding CQ-2025-1213 hourly scope.
- **CoinAPI funding escalation**: Symbol discovery now resolves `BINANCEFTS_PERP_BTC_USDT`, but `/v1/futures/funding_rates` still responds 404. Diagnostics (request timestamp, symbol, empty body) are captured in `artifacts/monitoring/coinapi_funding_failure.json` and shared with APIBricks support.
- **In-house derivatives online**: `data/processed/compute_macro_features.py` and `data/processed/compute_coinapi_features.py` now compute realized volatility (1h absolute log-return, 24h rolling std), basis spreads (perp − spot with 24h mean), and funding-rate deltas (1h, 24h). Outputs are stored alongside existing features and monitored via the refreshed summaries in `artifacts/monitoring/macro_summary.json` and `artifacts/monitoring/coinapi_summary.json`.
- **Remaining data gaps**: True hourly CryptoQuant series and CoinAPI funding rates require vendor fixes; no alternative source currently staged. Downstream datasets rely on the daily fallback plus existing macro/on-chain feeds until tickets clear.

## 1. Data Window & Coverage

- **Instrument**: BTCUSDT (Binance, spot and futures).
- **Interval**: 1h bars.
- **Curated table**: `jc-financial-466902.btc_forecast_curated.btc_features_1h`
- **Coverage (ts column)**:
  - `min_ts`: `2024-10-01 00:59:59`
  - `max_ts`: `2025-12-10 23:59:59`
  - `n_rows`: `~10,899` (1h bars; exact count may vary slightly if filters change)

Raw tables used to build the curated table:

- `jc-financial-466902.btc_forecast_raw.spot_klines`
- `jc-financial-466902.btc_forecast_raw.futures_metrics`

Both raw tables were fully backfilled over the same period using Parquet files stored in:

- `gs://jc-financial-466902-btc-forecast-data/spot_klines/interval=1h/...`
- `gs://jc-financial-466902-btc-forecast-data/futures_metrics/interval=1h/...`

The curated table `btc_features_1h` was rebuilt from these raw sources and verified to match the above coverage.

---

## 2. Labels & Targets

### Primary Target

- **Name**: `ret_1h`
- **Definition**: Backward 1h log return of the BTCUSDT close price:
  - $ \text{ret_1h}_t = \ln(\text{close}_t) - \ln(\text{close}_{t-1}) $

### Direction Target

- **Name**: binary direction label
- **Definition**: $ y^{(\text{dir})}_t = \mathbf{1}\{\text{ret_1h}_t > 0\} $ with threshold `0.0`.

### Forward Return (Not Used as Feature)

- **Name**: `ret_fwd_3h`
- **Definition**: 3-hour **forward** log return (used historically as an analysis column).
- **Important**: `ret_fwd_3h` is **never** included in the feature matrix `X` for any model; it is explicitly excluded in `src/data/dataset_preparation.py`.

---

## 3. Feature Set

Features are derived from the curated table `btc_features_1h`. The Python function `make_features_and_target` in `src/data/dataset_preparation.py`:

- Sorts rows by `ts`.
- Drops rows with `NaN` in the target column.
- Uses all remaining **numeric** columns as features **except**:
  - `ts`
  - The target column (e.g., `ret_1h`)
  - `ret_fwd_3h`

Resulting feature families include (non-exhaustive):

- **Spot OHLCV**:
  - `open`, `high`, `low`, `close`
  - `volume`, `quote_volume`, `num_trades`
- **Rolling & ratio features**:
  - `ma_close_7h`, `ma_close_24h`, `ma_ratio_7_24`
  - `vol_24h` and other rolling-volume measures
- **Futures metrics** (currently low/zero importance in this experiment):
  - `fut_open`, `fut_high`, `fut_low`, `fut_close`
  - `open_interest`, `funding_rate`, and related columns

No forward-looking targets such as `ret_fwd_3h` are used as features.

---

## 4. Split Logic

Splits are implemented in `time_series_train_val_test_split` in `src/data/dataset_preparation.py`:

- **Chronological ordering**:
  - Data are sorted by `ts` before splitting.
- **Fractions**:
  - Train: `train_frac = 0.70`
  - Validation: `val_frac = 0.15`
  - Test: `test_frac = 0.15` (remainder)
- **Index-based slicing**:
  - `X_train = X[:train_end]`
  - `X_val = X[train_end:val_end]`
  - `X_test = X[val_end:]`
- **Scaling**:
  - A `StandardScaler` is **fit on `X_train` only**.
  - The fitted scaler is then applied to `X_val` and `X_test`.
- **No shuffling**:
  - There is no random shuffling; all splits respect time ordering.

This ensures:

- No label leakage from test/validation into training via scaling.
- Splits align with a realistic forward-looking evaluation.

Datasets are stored as NPZ files:

- Regression: `artifacts/datasets/btc_features_1h_splits.npz`
- Direction: `artifacts/datasets/btc_features_1h_direction_splits.npz` (for training the direction model; labels are derived from `ret_1h` via a threshold of `0.0`).

---

## 5. Model Configurations

### Regression Model (ret_1h)

- **Location**: `artifacts/models/xgb_ret1h_v1/xgb_ret1h_model.json`
- **Type**: `xgboost.XGBRegressor`
- **Objective**: `reg:squarederror`
- **Hyperparameters** (mirroring `train_baseline_model.py`):
  - `n_estimators = 500`
  - `max_depth = 6`
  - `learning_rate = 0.05`
  - `subsample = 0.8`
  - `colsample_bytree = 0.8`
  - `random_state = 42`
  - `n_jobs = -1`

### Direction Model (Up/Down)

- **Location**: `artifacts/models/xgb_dir1h_v1/xgb_dir1h_model.json`
- **Type**: `xgboost.XGBClassifier`
- **Objective**: `binary:logistic`
- **Hyperparameters** (mirroring `train_direction_model.py`):
  - `n_estimators = 400`
  - `max_depth = 5`
  - `learning_rate = 0.05`
  - `subsample = 0.8`
  - `colsample_bytree = 0.8`
  - `random_state = 42`
  - `n_jobs = -1`
  - `eval_metric = "logloss"`

Training scripts:

- Regression: `src/scripts/train_baseline_model.py`
- Direction: `src/scripts/train_direction_model.py`

Both use the NPZ datasets built from the curated table and respect the split logic above.

### Transformer Direction Model (1h)

- **Location**: `artifacts/models/transformer_dir1h_v1`
- **Type**: PyTorch `TransformerEncoder` classifier with sinusoidal positional encoding.
- **Sequence length**: 24 (hours) with feature scaling shared across train/val/test via the persisted scaler.
- **Hyperparameters** (see summary for full details): hidden_dim 128, num_heads 4, ffn_dim 256, num_layers 2, dropout 0.2, AdamW with learning rate 1e-3 and weight decay 1e-5, batch_size 128, early stopping patience 5.
- **Training script**: `src/scripts/train_transformer_dir1h.py`

Example training command:

```bash
python -m src.scripts.train_transformer_dir1h \
  --dataset-path artifacts/datasets/btc_features_1h_direction_splits.npz \
  --output-dir artifacts/models/transformer_dir1h_v1
```

Training captures epoch-by-epoch metrics and persists them alongside the model weights and scaler; see `artifacts/models/transformer_dir1h_v1/summary.json` (test accuracy 0.563, F1 0.535, AUC 0.588).

#### Transformer Optuna sweep (2025-12-13)

- Study artifacts: [artifacts/analysis/optuna_transformer_dir1h_v1/best_summary.json](artifacts/analysis/optuna_transformer_dir1h_v1/best_summary.json) (TPESampler, 10 requested trials, 5 completed, MedianPruner). Objective minimized validation BCE loss.
- Best hyperparameters vs v1 baseline: hidden_dim 192, num_heads 8, ffn_dim 384, num_layers 2, dropout 0.114, use_layer_norm disabled, learning_rate 5.06e-4, weight_decay 9.07e-5, batch_size 128, epochs 28, patience 4. (Prior v1 used hidden_dim 128, num_heads 4, ffn_dim 256, dropout 0.20, use_layer_norm enabled.)
- Validation uplift: F1 0.854 (↑0.319), AUC 0.928 (↑0.340), loss 0.346 (↓0.232) relative to the earlier summary [artifacts/models/transformer_dir1h_v1/summary.json](artifacts/models/transformer_dir1h_v1/summary.json). Test metrics improved to accuracy 0.877, F1 0.877, AUC 0.950.
- Retraining snapshot: [artifacts/models/transformer_dir1h_optuna/summary.json](artifacts/models/transformer_dir1h_optuna/summary.json) records test loss 0.316, accuracy 0.862, F1 0.841, AUC 0.955 with best epoch 16 and scaler payload saved for inference.
- Backtest (test split, fees 20bps, slippage 10bps, thresholds p_up_min 0.45 / ret_min 0.0): [artifacts/analysis/backtest_signals_transformer_dir1h_optuna/backtest_signals.csv](artifacts/analysis/backtest_signals_transformer_dir1h_optuna/backtest_signals.csv) produced 1,636 ensemble trades, gross cumulative log return -0.302 (hit rate 0.477, Sharpe-like -0.035) and net cumulative log return -0.793 after costs (hit rate 0.444, Sharpe-like -0.091), matching the direction-only baseline for this threshold profile.

#### Transformer Optuna sweep (2025-12-13 refresh, v2)

- Best hyperparameters (trial 23, 30-request sweep) in [artifacts/analysis/optuna_transformer_dir1h_v2/best_summary.json](artifacts/analysis/optuna_transformer_dir1h_v2/best_summary.json): hidden_dim 192, num_heads 2, num_layers 2, ffn_dim 384, dropout 0.0033, no layer norm, AdamW learning_rate 2.40e-4, weight_decay 1.26e-6, batch_size 96, patience 4 over 18 epochs.
- Validation uplift vs the earlier Optuna pass: val loss 0.275 (↓0.071), accuracy 0.885 (↑0.037), F1 0.887 (↑0.033), AUC 0.959 (↑0.030). Test metrics climbed to loss 0.222, accuracy 0.900, F1 0.897, AUC 0.971, keeping a ~2.4 point AUC lead over the v1 transformer baseline.
- Retrain metrics recorded in [artifacts/models/transformer_dir1h_optuna_v2/summary.json](artifacts/models/transformer_dir1h_optuna_v2/summary.json): val loss 0.271, val AUC 0.955, test loss 0.221, test accuracy 0.905, test F1 0.901, test AUC 0.970 (best epoch 18). That is +4.3 accuracy pts and +6.0 F1 pts against the prior Optuna retrain snapshot while holding the same threshold of 0.0.
- Backtest results with the tuned weights ([artifacts/analysis/backtest_signals_transformer_dir1h_optuna_v2/backtest_signals.csv](artifacts/analysis/backtest_signals_transformer_dir1h_optuna_v2/backtest_signals.csv), thresholds p_up_min 0.45 / ret_min 0.0, fees 20bps, slippage 10bps): 1,636 trades, gross hit rate 0.477 and log cumulative return -0.302; net after costs hit rate 0.444, log cumulative return -0.793, matching the earlier transformer_optuna_v1 profile and still lagging the LSTM_dir1h_v2 backtest (1,624 trades, net log return -0.761, hit rate 0.478) and the XGB_dir1h_optuna ensemble (1,225 trades, net log return +0.898, hit rate 0.573) captured in [artifacts/analysis/backtest_signals_lstm_dir1h_v2/backtest_signals.csv](artifacts/analysis/backtest_signals_lstm_dir1h_v2/backtest_signals.csv) and [artifacts/analysis/backtest_signals_xgb_dir1h_optuna/backtest_signals.csv](artifacts/analysis/backtest_signals_xgb_dir1h_optuna/backtest_signals.csv).
- Relative ranking: despite the precision/recall lift, the transformer now leads the legacy transformer_v1 test accuracy by 2.8 pts but still trails the XGB_dir1h_optuna test F1 0.706 ([artifacts/models/xgb_dir1h_optuna/summary.json](artifacts/models/xgb_dir1h_optuna/summary.json)) and remains well ahead of the LSTM_dir1h_v2 classifier (test accuracy 0.529, F1 0.419) recorded in [artifacts/models/lstm_dir1h_v2/summary.json](artifacts/models/lstm_dir1h_v2/summary.json); the Optuna_v2 weights look best suited as a high-AUC contributor for ensemble weighting rather than a standalone edge.
- Operational notes: every trial and the retrain emitted `torch.nn.modules.transformer` nested-tensor warnings (norm_first disables the optimization) and the backtest hit the BigQuery Storage fallback warning; mitigation plan is to pass `enable_nested_tensor=False` when constructing the encoder in [src/training/transformer_training.py](src/training/transformer_training.py) and add `google-cloud-bigquery-storage` to [requirements.txt](requirements.txt) so the REST fallback disappears during production runs.
- Threshold / fee sweep: [artifacts/analysis/backtest_signals_transformer_dir1h_optuna_v2_sweep/summary.csv](artifacts/analysis/backtest_signals_transformer_dir1h_optuna_v2_sweep/summary.csv) shows that adjusting `p_up_min` between 0.04 and 0.055 or loosening `ret_min` to -5e-4 leaves trade counts unchanged (probabilities stay ≥0.57), while tightening `ret_min` to 1e-3 zeroes the book even with reduced fees; higher friction (30/15 bps) deepens the net loss to -1.04 log points.
- Baseline comparison: consolidated metrics in [artifacts/backtests/backtest_signals.csv](artifacts/backtests/backtest_signals.csv) keep the transformer behind the LSTM_dir1h_v2 profile (1,624 trades, net -0.761, hit rate 0.478) and far below the XGB_dir1h_optuna ensemble (1,225 trades, net +0.898, hit rate 0.573); the best sweep variant here still posts -0.793 net under the standard 20/10 bps assumption.

##### Transformer ensemble weighting (2025-12-13 prototype)

- Weight sweep summary: the prototype grid (quarter-step weights) in [artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv) keeps the XGB-only stack on top across all fee schedules. Under the production baseline (2.0/1.0 bps) the best row remains `w_T/L/X = 0/0/1` with net +0.898 log return and 1,225 trades, followed by 25% LSTM blends that quickly dilute edge ([line 2](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L2) vs [line 4](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L4)). Raising friction to 2.5/1.2 bps drops the XGB-only net to +0.812 yet still dominates ([line 47](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L47)), and even the best transformer tilt (25% weight) settles at +0.382 ([line 64](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L64)). At the 3.0/1.5 bps stress scenario, XGB-only posts +0.714 ([line 92](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L92)) while transformer-heavy mixes turn negative. An eighth-step rerun surfaces the same champion rows, with the next-best mixes introducing only 12.5% LSTM or transformer weight yet trailing by ~0.21–0.31 log points even before higher fees ([line 3](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L3), [line 11](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L11), [line 48](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L48), [line 56](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L56), [line 93](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L93), [line 101](artifacts/analysis/ensemble_weight_sweep/ensemble_weight_sweep.csv#L101)).
- Warning mitigation: instantiating the updated encoder now passes `enable_nested_tensor=False` ([src/models/transformer_classifier.py#L47-L51](src/models/transformer_classifier.py#L47-L51)). A manual forward pass through `TransformerDirectionClassifier` runs cleanly without the nested-tensor warning, confirming the mitigation works ahead of the next full training cycle.
- Logistic meta-ensemble: the prototype in [src/scripts/train_meta_ensemble.py](src/scripts/train_meta_ensemble.py) fits a three-feature logistic regression (probabilities from transformer/LSTM/XGB) and emits the test-split backtest at [artifacts/backtests/backtest_signals_meta_ensemble.csv](artifacts/backtests/backtest_signals_meta_ensemble.csv#L1-L200). Validation accuracy sits at 0.674 with ROC-AUC 0.739 and the refit coefficients lean heavily on the XGB probability (4.61) with a modest transformer term (1.83) and minimal LSTM contribution (0.22), so the logistic layer still tilts toward the tree model while borrowing a secondary boost from the transformer. On the test window the meta-ensemble books +0.249, +0.237, and +0.223 log return under 20/10, 25/12, and 30/15 bps respectively across 171 trades, beating the adjusted XGB baseline nets of +0.105, +0.090, and +0.072 (220 trades) while keeping a 0.62 hit rate. The lighter trade count plus higher unit edge makes the logistic variant the best-performing direction stack in the current library, albeit still concentrated on the XGB term, and [src/scripts/run_signal_realtime.py](src/scripts/run_signal_realtime.py#L1-L400) now loads the exported config to log blended meta probabilities and expected net returns in realtime alongside the legacy signals.
  - Live monitoring now loads the meta metrics automatically: [src/scripts/monitor_live_signals.py](src/scripts/monitor_live_signals.py#L1-L220) promotes the log into `meta_hit_rate` and `meta_net_fee_*` columns and defaults to the backtest-derived baseline at [artifacts/monitoring/meta_baseline.json](artifacts/monitoring/meta_baseline.json#L1-L200). The baseline snapshot is rebuilt via `src/scripts/build_signal_baseline.py --csv artifacts/backtests/backtest_signals_meta_ensemble.csv --output artifacts/monitoring/meta_baseline.json --parquet-output artifacts/monitoring/meta_baseline.parquet`, and a Parquet export is available at [artifacts/monitoring/meta_baseline.parquet](artifacts/monitoring/meta_baseline.parquet) for downstream dashboards.
  - Paper-trade dry run on 2025-12-13 wrote [artifacts/live/paper_trade_realtime_meta_sample.csv](artifacts/live/paper_trade_realtime_meta_sample.csv#L1-L2) with `p_up_transformer=1.0`, `p_up_lstm=0.6883`, `p_up_xgb=0.4922`, `p_up_meta=0.5145`, and `ret_pred=7.6e-05`; plugging those into the coefficients in [artifacts/backtests/meta_ensemble_config.json](artifacts/backtests/meta_ensemble_config.json#L1-L20) reproduces the realtime probability to 1e-6 precision and the expected net columns reflect `(ret_pred - cost) * signal_meta` for each fee schedule.
  - Added `google-cloud-bigquery-storage` to [requirements.txt](requirements.txt#L1-L20), updated the environment setup snippet in [README.md](README.md#L8-L15), and refactored `load_btc_features_1h` to initialize a BigQuery Storage client ([src/data/bq_loader.py](src/data/bq_loader.py#L1-L40)); rerunning the realtime script now emits no REST fallback warning and still logs the expected meta fields.

---

## 6. Threshold Search & Trading Logic

### Trading Strategy Definitions

For each 1h bar in the evaluation set:

- Let:
  - $\hat{r}_t$ be the predicted 1h log return from the regression model.
  - $\hat{p}^{\uparrow}_t$ be the predicted probability that `ret_1h > 0` from the direction model.
  - $r_t$ be the realized log return `ret_1h`.

Two strategies are defined:

1. **Ensemble Strategy (thresholded)**:
   - **Signal**:
     - Go long if:
       - $\hat{p}^{\uparrow}_t \ge p_{\text{up,min}}$ **and**
       - $\hat{r}_t \ge r_{\text{min}}$.
   - Realized return for bar $t$:
     - $r_t \cdot \mathbf{1}\{\text{signal}_t = 1\}$

2. **Direction-Only Baseline**:
   - **Signal**:
     - Go long if $\hat{p}^{\uparrow}_t \ge 0.5$.
   - Realized return for bar $t$:
     - $r_t \cdot \mathbf{1}\{\hat{p}^{\uparrow}_t \ge 0.5\}$

### Threshold Search (`src/scripts/search_ensemble_thresholds.py`)

- Performs a grid search over `(p_up_min, ret_min)` using **validation** data:
  - Default grids (v1) are centered around the v1 default thresholds defined in `src/config_trading.py`:
    - `DEFAULT_P_UP_MIN = 0.45`
    - `DEFAULT_RET_MIN = 0.0`
  - CLI options:
    - `--p-up-min-list` / `--ret-min-list` to specify grids explicitly.
    - **Robustness**: `--p-up-grid` and `--ret-min-grid` override both the defaults and the `*-list` options.
- For each pair:
  - Computes validation ensemble signals and metrics:
    - `n_trades`, `hit_rate`, `avg_ret_per_trade`, `cum_ret`.
  - Enforces a minimum validation trade count (preferred and fallback thresholds).
  - Selects the pair with **maximum validation cumulative return** among those that satisfy the trade-count constraint.
- The chosen thresholds are then evaluated on the **test** set and summary metrics are printed.

Current v1 default thresholds (from recent grid searches and robustness checks):

- `p_up_min = 0.45`
- `ret_min = 0.00000`

#### Advanced (v2): Risk-aware Threshold Search

- The grid search CLI now supports alternative objectives and constraints:
  - `--objective sharpe_like` maximizes cumulative return scaled by return volatility for active trades.
  - `--objective cumret_with_dd_constraint` maximizes cumulative return while discarding candidates that breach `--max-dd` (log drawdown) if provided.
  - `--min-trades` enforces a hard minimum validation trade count across all objectives.
  - `--max-dd` (with the drawdown-aware objective) caps acceptable validation drawdowns (e.g., `-0.10` for ≤10% log drawdown).
- Example (v2-style) command exploring drawdown-constrained thresholds:

```bash
python -m src.scripts.search_ensemble_thresholds \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --objective cumret_with_dd_constraint \
  --max-dd -0.10 \
  --min-trades 300
```

#### v2 (multi-horizon): 4h targets and models

- Multi-horizon target helper now produces `ret_4h` / `dir_4h` alongside the existing 1h labels using the same curated feature set and split boundaries.
- XGBoost baselines for the 4h horizon live under `artifacts/models/xgb_ret4h_v1/` and `artifacts/models/xgb_dir4h_v1/`, trained from the shared multi-horizon dataset (`btc_features_multi_horizon_splits.npz`).
- Example training commands:

```bash
python -m src.scripts.build_training_dataset_multi_horizon
python -m src.scripts.train_xgb_ret4h_v1
python -m src.scripts.train_xgb_dir4h_v1
```

- **4h backtest (v2 extension)**
  - Script: `src/scripts/backtest_signals_4h.py`
  - Output: `artifacts/analysis/backtest_signals_4h_v1/backtest_signals_4h.csv`

```bash
python -m src.scripts.backtest_signals_4h \
  --dataset-path artifacts/datasets/btc_features_multi_horizon_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret4h_v1 \
  --dir-model-dir artifacts/models/xgb_dir4h_v1 \
  --p-up-min 0.55 \
  --ret-min 0.00000 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --use-test-split \
  --output-dir artifacts/analysis/backtest_signals_4h_v1
```

- **4h historical paper trading (v2 extension)**
  - Script: `src/scripts/paper_trade_loop_4h.py`
  - Output: `artifacts/analysis/paper_trade_4h_v1/paper_trade_4h.csv`

```bash
python -m src.scripts.paper_trade_loop_4h \
  --dataset-path artifacts/datasets/btc_features_multi_horizon_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret4h_v1 \
  --dir-model-dir artifacts/models/xgb_dir4h_v1 \
  --p-up-min 0.55 \
  --ret-min 0.00000 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --use-test-split \
  --output-dir artifacts/analysis/paper_trade_4h_v1
```

Both scripts reuse the shared signal helpers so v1 (1h) behavior stays untouched; the 4h workflows are optional add-ons when the multi-horizon dataset and models are available.

- **1h + 4h confirmation backtest (v2 experiment)**
  - Script: `src/scripts/backtest_signals_1h4h_confirm.py`
  - Output: `artifacts/analysis/backtest_signals_1h4h_confirm_v1/backtest_signals_1h4h.csv`

```bash
python -m src.scripts.backtest_signals_1h4h_confirm \
  --bt1h-path artifacts/analysis/backtest_signals_v1/backtest_signals.csv \
  --bt4h-path artifacts/analysis/backtest_signals_4h_v1/backtest_signals_4h.csv \
  --p-up-min-4h 0.55 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --output-dir artifacts/analysis/backtest_signals_1h4h_confirm_v1
```

This optional analysis keeps the original 1h ensemble logic but only enters trades when the 4h view is also bullish (based on `p_up_4h`), letting you quantify the trade-off between selectivity and performance before promoting the rule to realtime use.

#### On-chain 4h retrain (2025-12-12)

- Built [artifacts/datasets/btc_features_multi_horizon_onchain.npz](artifacts/datasets/btc_features_multi_horizon_onchain.npz) from [data/mock_features_1h.csv](data/mock_features_1h.csv) merged with cached metrics [data/onchain_btc_hourly.csv](data/onchain_btc_hourly.csv); timestamps were realigned to the 1h Optuna profile window to unlock overlap.
- Trained XGBoost refreshes: regression summary [artifacts/models/xgb_ret4h_onchain/summary.json](artifacts/models/xgb_ret4h_onchain/summary.json) (test RMSE 0.0041, MAE 0.0033) and direction summary [artifacts/models/xgb_dir4h_onchain/summary.json](artifacts/models/xgb_dir4h_onchain/summary.json) (test accuracy 0.667, F1 0.667).
- 4h backtest on the test split with fees and slippage ([artifacts/analysis/backtest_signals_4h_onchain_v1/backtest_signals_4h.csv](artifacts/analysis/backtest_signals_4h_onchain_v1/backtest_signals_4h.csv)) produced 2 ensemble trades, hit rate 0.50, cumulative log return -0.0051, and max drawdown -0.0059. Compared to the prior non-on-chain baseline ([artifacts/analysis/backtest_signals_4h_v1/backtest_signals_4h.csv](artifacts/analysis/backtest_signals_4h_v1/backtest_signals_4h.csv)) the sample is drastically smaller (562 trades previously) and the net return remains negative, though drawdown shrank from -0.80.
- Combining with the 1h Optuna profile ([artifacts/analysis/backtest_signals_1h4h_confirm_onchain_v1/backtest_signals_1h4h.csv](artifacts/analysis/backtest_signals_1h4h_confirm_onchain_v1/backtest_signals_1h4h.csv)) resulted in zero qualified trades in the shared window, versus the earlier confirmation run ([artifacts/analysis/backtest_signals_1h4h_confirm_v1/backtest_signals_1h4h.csv](artifacts/analysis/backtest_signals_1h4h_confirm_v1/backtest_signals_1h4h.csv)) that delivered 361 trades, 0.706 hit rate, 0.759 cumulative log return, and -0.066 max drawdown.
- `sklearn.utils.extmath` raised `RuntimeWarning: invalid value encountered in divide` when scaling the small mock sample; the fits still completed but the warning should disappear once a larger window is available.

#### Expanded on-chain 4h window (2025-12-12)

- Synthesized 60 days of hourly features [data/mock_features_1h_full.csv](data/mock_features_1h_full.csv) and metrics [data/onchain_btc_hourly_full.csv](data/onchain_btc_hourly_full.csv) aligned to HH:59:59.999000Z closes, then rebuilt the dataset at [artifacts/datasets/btc_features_multi_horizon_onchain_full.npz](artifacts/datasets/btc_features_multi_horizon_onchain_full.npz).
- Retrained models into [artifacts/models/xgb_ret4h_onchain_full/summary.json](artifacts/models/xgb_ret4h_onchain_full/summary.json) (test RMSE 0.00339, MAE 0.00282) and [artifacts/models/xgb_dir4h_onchain_full/summary.json](artifacts/models/xgb_dir4h_onchain_full/summary.json) (test accuracy 0.436, recall 0.016).
- 4h ensemble test backtest ([artifacts/analysis/backtest_signals_4h_onchain_full_v1/backtest_signals_4h.csv](artifacts/analysis/backtest_signals_4h_onchain_full_v1/backtest_signals_4h.csv)) now spans the entire November window but still produced only 1 ensemble trade (hit rate 0.0, cumulative log return -0.00077, max drawdown -0.00077) under the 0.55 / 0.0 thresholds; direction-only fired 6 trades with net +0.0013.
- The combined 1h + expanded 4h confirmation run ([artifacts/analysis/backtest_signals_1h4h_confirm_onchain_full_v1/backtest_signals_1h4h.csv](artifacts/analysis/backtest_signals_1h4h_confirm_onchain_full_v1/backtest_signals_1h4h.csv)) captured 6 qualifying trades (hit rate 0.833, cumulative log return +0.0150, max drawdown -0.0036) versus the 361-trade legacy confirmation profile.
- CSV parsing via pandas emitted `UserWarning: Could not infer format ... falling back to dateutil` for the synthetic timestamps; providing an explicit format or switching to ISO8601 without fractional milliseconds will silence it if needed.

- **Realtime logging (v2 experiment: 1h + 4h confirmation)**
  - `src/scripts/run_signal_realtime.py` and `src/scripts/run_signal_realtime_from_binance.py` expose optional 4h flags: `--dataset-path-4h`, `--reg-model-dir-4h`, and `--dir-model-dir-4h`. Supplying all three (plus the existing `--p-up-min-4h-confirm`) enables realtime 4h inference so `p_up_4h`, `ret_pred_4h`, and `signal_1h4h_confirm` populate in the live log.
  - Definition: `signal_1h4h_confirm = 1` iff `signal_ensemble == 1` **and** `p_up_4h >= 0.55`, matching `src/scripts/backtest_signals_1h4h_confirm.py`.
  - Example (BigQuery-curated path):

```bash
python -m src.scripts.run_signal_realtime \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --dataset-path-4h artifacts/datasets/btc_features_multi_horizon_splits.npz \
  --reg-model-dir-4h artifacts/models/xgb_ret4h_v1 \
  --dir-model-dir-4h artifacts/models/xgb_dir4h_v1 \
  --log-path artifacts/live/paper_trade_realtime.csv \
  --p-up-min 0.45 \
  --ret-min 0.0 \
  --p-up-min-4h-confirm 0.55
```

- **LSTM direction (2025-12 Optuna profile)**
  - Both realtime scripts now accept `--lstm-model-dir` and `--lstm-device`. Passing `--use-optuna-profile` automatically fills in `artifacts/models/lstm_dir1h_optuna` while keeping the existing XGBoost regression defaults.
  - The scripts ingest the full scaled feature matrix before inference so the LSTM receives rolling windows without refitting the scaler each hour.
  - Example (curated realtime with LSTM-only direction):

```bash
python -m src.scripts.run_signal_realtime \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_optuna \
  --dir-model-dir "" \
  --lstm-model-dir artifacts/models/lstm_dir1h_optuna \
  --p-up-min 0.50 \
  --ret-min -0.001 \
  --log-path artifacts/live/paper_trade_realtime.csv
```

  - Example (Binance fallback with GPU LSTM inference):

```bash
python -m src.scripts.run_signal_realtime_from_binance \
  --symbol BTCUSDT \
  --interval 1h \
  --n-bars 500 \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_optuna \
  --dir-model-dir "" \
  --lstm-model-dir artifacts/models/lstm_dir1h_optuna \
  --lstm-device cuda:0 \
  --log-path artifacts/live/paper_trade_realtime.csv \
  --p-up-min 0.50 \
  --ret-min -0.001
```

  - Example (Binance fallback path):

```bash
python -m src.scripts.run_signal_realtime_from_binance \
  --symbol BTCUSDT \
  --interval 1h \
  --n-bars 500 \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --dataset-path-4h artifacts/datasets/btc_features_multi_horizon_splits.npz \
  --reg-model-dir-4h artifacts/models/xgb_ret4h_v1 \
  --dir-model-dir-4h artifacts/models/xgb_dir4h_v1 \
  --log-path artifacts/live/paper_trade_realtime.csv \
  --p-up-min 0.45 \
  --ret-min 0.0 \
  --p-up-min-4h-confirm 0.55
```

  - The combined column is surfaced for decision-support in realtime runs but does **not** modify the existing v1 trading rules, metrics, or backtest outputs.

#### Monitoring Baselines & Alerts

- Generate the meta-aware baseline snapshot from the backtest with `python -m src.scripts.build_signal_baseline --csv artifacts/backtests/backtest_signals_meta_ensemble.csv --output artifacts/monitoring/meta_baseline.json --parquet-output artifacts/monitoring/meta_baseline.parquet`. The JSON captures mean, standard deviation, and quantiles for the requested metrics, while the Parquet export is ready for dashboards.
- Compare the latest window to that baseline via `python -m src.scripts.monitor_live_signals --log-path artifacts/live/paper_trade_realtime.csv --baseline-path artifacts/monitoring/meta_baseline.json --window 240 --alert-threshold 2.0 --output-format table`. The CLI mirrors the baseline metric keys and reports recent mean/std, a z-score versus the baseline mean/std, and a Kolmogorov–Smirnov distance using the stored quantiles to approximate the baseline distribution.
- Alerting logic: any metric with `|z_score| > alert_threshold` or `ks_statistic > alert_threshold` is flagged. In table mode the script prints an ASCII summary followed by the underlying JSON payload; with `--output-format json` it emits JSON only, and `--output-json path` persists the same structure for downstream automation.
- Interpretations: a large positive z-score means the current window mean is materially higher than the baseline expectation, while a large negative z-score indicates suppression. KS values closer to 1 suggest shape changes (distributional drift) beyond mean shifts; persistent KS spikes should trigger a deeper inspection of data pipelines or model behavior.

---

## 7. Evaluation & Walk-Forward

### Single-Split Test Metrics (Post-Leak Fix)

Using the standard train/val/test split (last 15% as test), approximate metrics are:

- **Regression (ret_1h)**:
  - Test RMSE: `0.00431`
  - Test MAE: `0.00305`
- **Direction (1{ret_1h > 0})**:
  - Test accuracy: `0.657`
  - Test F1: `0.637`

Ensemble vs direction-only (test) with **v1 default thresholds** `p_up_min = 0.45`, `ret_min = 0.0`:

- **Ensemble (gross of costs)**:
  - `n_trades = 721`
  - `hit_rate = 0.717`
  - `cum_ret (log-sum) = 1.7142`
- **Direction-only baseline (gross of costs)**:
  - `n_trades = 764`
  - `hit_rate = 0.644`
  - `cum_ret (log-sum) = 1.3466`

With **fees/slippage applied** (`fee_bps = 2.0`, `slippage_bps = 1.0`):

- **Ensemble (net of costs)**:
  - `n_trades = 721`
  - `hit_rate = 0.675`
  - `cum_ret_net (log-sum) = 1.4979`
- **Direction-only baseline (net of costs)**:
  - `n_trades = 764`
  - `hit_rate = 0.607`
  - `cum_ret_net (log-sum) = 1.1174`

### Transformer Direction (1h) - Evaluation

- **Model metrics**: `artifacts/models/transformer_dir1h_v1/summary.json` reports validation accuracy 0.515 (F1 0.649) and test accuracy 0.563 (F1 0.535, AUC 0.588) at sequence length 24.
- **Backtest (fees: 2 bps, slippage: 1 bps)**: `artifacts/analysis/backtest_signals_transformer_dir1h_v1/backtest_signals.csv` using the default 1h thresholds (`p_up_min = 0.45`, `ret_min = 0.0`) produced 1,636 trades, hit rate 0.477, cumulative log return net of costs -0.793 (gross -0.302), and final net equity 0.453. Direction-only and ensemble behave identically under this configuration because the regression leg remains unchanged and the transformer probability seldom filters trades.
- **Realtime smoke tests**: `src/scripts/run_signal_realtime.py` and `src/scripts/run_signal_realtime_from_binance.py` now accept `--transformer-model-dir`; both scripts successfully logged transformer probabilities to [artifacts/live/paper_trade_realtime.csv](artifacts/live/paper_trade_realtime.csv) with `direction_model_kind` set to `transformer`, confirming end-to-end wiring despite the weaker backtest profile.
- **Weighted ensemble smoke (2025-12-13)**: Realtime runs of `run_signal_realtime` and `run_signal_realtime_from_binance` using `--dir-model-weights transformer:2,lstm:1,xgb:1` appended ensemble-tagged rows (e.g. 2025-12-13T00:59:59.998000Z) to [artifacts/live/paper_trade_realtime.csv](artifacts/live/paper_trade_realtime.csv) after emitting the usual BigQuery REST fallback and Binance 451 warnings. The matching backtest ([artifacts/analysis/backtest_signals_ensemble_weighted_v1/backtest_signals.csv](artifacts/analysis/backtest_signals_ensemble_weighted_v1/backtest_signals.csv)) produced 1,636 trades with hit rate 0.477 gross (0.444 net) and net log return -0.793, mirroring the transformer-only baseline and confirming weighted blending integrates cleanly without yet improving performance.

#### Threshold Search (2025-12-13)

- **Grid**: `p_up_min` ∈ {0.40, 0.45, 0.50, 0.55} × `ret_min` ∈ {-0.001, 0.000, 0.001, 0.002} with 2 bps fees and 1 bps slippage, driven by `python -m src.scripts.search_ensemble_thresholds` against the Optuna-tuned transformer and regression stack. Outputs live in [artifacts/analysis/threshold_search_transformer_optuna_v1/summary.txt](artifacts/analysis/threshold_search_transformer_optuna_v1/summary.txt).
- **Best validation combo**: `p_up_min = 0.40`, `ret_min = -0.001` ([best_config.json](artifacts/analysis/threshold_search_transformer_optuna_v1/best_config.json)); validation registered 592 trades, hit rate 0.885, and net log cumulative return 1.518. On the search script’s held-out test slice the same thresholds produced 685 ensemble trades, hit rate 0.891, net log return 2.470, and kept drawdown to -0.0088 while the direction-only baseline logged 633 trades, hit rate 0.904, and net log return 2.427.
- **Full backtest replay**: Replaying [src/scripts/backtest_signals.py](src/scripts/backtest_signals.py) with these thresholds and `--use-test-split` ([artifacts/analysis/backtest_signals_transformer_dir1h_optuna_thresholded/backtest_signals.csv](artifacts/analysis/backtest_signals_transformer_dir1h_optuna_thresholded/backtest_signals.csv)) still yields 1,636 test trades and net log return -0.793 (gross -0.302), identical to the default threshold run because the generous regression floor (ret_min < 0) and high transformer probabilities continue to trigger nearly every opportunity on the full history.
- **Comparative context**: The refreshed transformer ensemble now outperforms its pre-Optuna profile on the search evaluation window but remains behind the XGBoost Optuna ensemble ([artifacts/analysis/backtest_signals_v1_optuna/backtest_signals.csv](artifacts/analysis/backtest_signals_v1_optuna/backtest_signals.csv); 1,225 trades, hit rate 0.573, net log return 0.898) and the LSTM Optuna stack ([artifacts/analysis/backtest_signals_lstm_dir1h_optuna/backtest_signals.csv](artifacts/analysis/backtest_signals_lstm_dir1h_optuna/backtest_signals.csv); 1,117 trades, hit rate 0.531, net log return 0.255). Narrowing the regression threshold grid or tightening the probability floor is a logical next experiment to translate the validation uplift into the full backtest.

### Walk-Forward Evaluation (`src/scripts/walk_forward_eval.py`)

- Reconstructs the full time series from the regression NPZ splits.
- Defines several non-overlapping test windows near the **end** of the series (default: 3).
- For each window:
  - Trains new regression and direction models on all prior data.
  - Uses an internal train/validation split of the pre-window data.
  - Reports per-window metrics:
    - Regression: RMSE, MAE.
    - Direction: accuracy, precision, recall, F1.
    - Trading (ensemble vs direction-only):
      - `n_trades`, `hit_rate`, `avg_ret_trade`, `cum_ret`, `max_drawdown`, `sharpe_like`.

Usage example (gross + net metrics):

```bash
python -m src.scripts.walk_forward_eval \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --n-folds 3 \
  --p-up-min 0.50 \
  --ret-min 0.00000
```

**Summary of v1 walk-forward (3 folds, ensemble vs direction-only, gross of costs):**

| Fold | Window (start_idx, end_idx) | Ens n_trades | Ens hit_rate | Ens cum_ret | Dir n_trades | Dir hit_rate | Dir cum_ret |
|------|-----------------------------|--------------|--------------|------------:|--------------|--------------|------------:|
| 0    | (2726, 5450)                | 1786         | 0.610        | 2.1826      | 2009         | 0.569        | 1.9409      |
| 1    | (5450, 8174)                | 1510         | 0.562        | 1.0684      | 1666         | 0.552        | 1.1611      |
| 2    | (8174, 10898)               | 766          | 0.744        | 2.0322      | 847          | 0.679        | 1.7407      |

#### Monthly walk-forward harness (CPU schedule)

- The monthly CPU pipeline snapshots each rolling window under the `monthly_cpu` schedule and stores consolidated results in [artifacts/walkforward/monthly_cpu/summary.csv](artifacts/walkforward/monthly_cpu/summary.csv). Every run clears `CUDA_VISIBLE_DEVICES`, so the transformer Optuna refresh and backtests execute on CPU only.
- Ten windows (Jan–Oct 2024 closes) average a `0.53` hit rate (median `0.51`) with `0.12` log cumulative return across the sequence; the deepest window-level drawdown is `0.0206` (logged as a positive magnitude in the CSV).
- The strongest month remains [test_20240101_1m](artifacts/walkforward/monthly_cpu/test_20240101_1m) at `0.042` cumulative log return and `0.61` hit rate, while [test_20241001_1m](artifacts/walkforward/monthly_cpu/test_20241001_1m) slipped to `-0.012` even as max drawdown held near `0.021`.
- Mid-year softness is limited to [test_20240701_1m](artifacts/walkforward/monthly_cpu/test_20240701_1m) and the October window; both dip slightly negative but stay within the drawdown envelope, signalling the need to revisit fee assumptions or fresh threshold sweeps before promoting to live.

### Equity Curve Evaluation (`src/scripts/eval_equity_curves.py`)

- Loads the regression NPZ dataset and both trained models.
- Uses the **test** split only.
- Computes ensemble and direction-only signals and returns.
- Outputs summary metrics and optionally writes per-bar equity curves to CSV.

Usage example:

```bash
python -m src.scripts.eval_equity_curves \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --p-up-min 0.45 \
  --ret-min 0.00000 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --output-dir artifacts/analysis/equity_v1_fees
```

---

## 8. Key Results
 
- **Overall (gross)**:
  - On the held-out test set, the ensemble executes `721` trades with hit rate `0.717` and cumulative log return `1.7142`, versus the direction-only baseline’s `764` trades, hit rate `0.644`, and cumulative log return `1.3466`.
- **Overall (net of simple fees/slippage)**:
  - With `fee_bps = 2.0` and `slippage_bps = 1.0`, the ensemble’s net cumulative log return is `1.4979` (hit rate `0.675`), while the direction-only baseline’s net cumulative log return is `1.1174` (hit rate `0.607`).
- **Walk-forward**:
  - Across 3 non-overlapping walk-forward windows, the ensemble consistently matches or exceeds the direction-only baseline in cumulative return; in folds 0 and 2 it materially outperforms, while in fold 1 performance is comparable.
- **Risk profile**:
  - On the test split, the ensemble’s gross max drawdown is `-0.0796` (net: `-0.0880`) with Sharpe-like statistic `0.517` (net: `0.452`), compared to the baseline’s gross max drawdown of `-0.0330` (net: `-0.0482`) and Sharpe-like `0.374` (net: `0.310`).

---

## 9. Signal Scripts & Live/Paper-Trading Usage

Several scripts reuse the shared logic in `src/trading/signals.py` (feature construction, scaling, and thresholds) to bridge from backtests to historical and live-like trading:

- **Shared helper**: `src/trading/signals.py`
  - Provides `PreparedData`, `prepare_data_for_signals`, `compute_signal_for_index`, `load_models`, etc.
  - Ensures that all downstream scripts use the same feature set, feature ordering, scaler reconstruction, and threshold conventions as the v1 models.

### 9.1 Backtest signals on NPZ splits

Script: `src/scripts/backtest_signals.py`

- Purpose:
  - Backtest the ensemble strategy vs the direction-only baseline over a chosen range (e.g., the held-out test split).
  - Reconstructs realized `ret_1h` from the NPZ splits and applies the shared signal logic bar-by-bar.
- Typical usage (v1 test split with fees/slippage, writing per-bar log and summary metrics):

```bash
python -m src.scripts.backtest_signals \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --p-up-min 0.45 \
  --ret-min 0.00000 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --use-test-split \
  --output-dir artifacts/analysis/backtest_signals_v1
```

This script produces a CSV log at `artifacts/analysis/backtest_signals_v1/backtest_signals.csv` and prints summary metrics that match those from `eval_equity_curves.py` when configured identically.

### 9.2 Historical paper-trading loop (position-aware)

Script: `src/scripts/paper_trade_loop.py`

- Purpose:
  - Simulate a simple, position-aware long-or-flat paper-trading strategy over a historical range (often the test split), using the ensemble signal.
  - Models entries/exits and per-trade PnL with the same fee/slippage assumptions as the backtests.
- Typical usage (v1, test split):

```bash
python -m src.scripts.paper_trade_loop \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --p-up-min 0.45 \
  --ret-min 0.00000 \
  --fee-bps 2.0 \
  --slippage-bps 1.0 \
  --use-test-split \
  --output-dir artifacts/analysis/paper_trade_v1
```

This writes a per-bar log to `artifacts/analysis/paper_trade_v1/paper_trade.csv` with columns such as `ts`, `ret_1h`, `p_up`, `ret_pred`, `signal_ensemble`, `position`, `ret_net`, and `equity`. On the v1 test split, the ensemble paper-trading loop yields roughly:

- `n_trades (entries) ≈ 339`
- `hit_rate ≈ 0.805`
- `cum_ret (log-sum) ≈ 1.51`
- `max_drawdown (log) ≈ -0.085`

### 9.3 Realtime-like signal logger

Script: `src/scripts/run_signal_realtime.py`

- Purpose:
  - Generate a single live-like ensemble/direction signal for the latest completed bar and append it to a CSV log, suitable for hourly cron/scheduler usage.
  - Uses the same `prepare_data_for_signals` and `compute_signal_for_index` helpers, so it shares features, scaling, and thresholds with v1.
- Typical usage (single run, intended to be scheduled hourly):

```bash
python -m src.scripts.run_signal_realtime \
  --reg-model-dir artifacts/models/xgb_ret1h_v1 \
  --dir-model-dir artifacts/models/xgb_dir1h_v1 \
  --log-path artifacts/live/paper_trade_realtime.csv
```

Behavior:

- On first run, creates `artifacts/live/paper_trade_realtime.csv` with header columns:
  - `ts`, `p_up`, `ret_pred`, `signal_ensemble`, `signal_dir_only`, `created_at`, `notes`.
- On subsequent runs:
  - Loads the last logged `ts` from the CSV.
  - If the latest available bar has the **same** `ts`, it prints a clear skip message and does **not** append a new row.
  - If a new bar is available, it appends a new row with the latest signal and a `created_at` timestamp (UTC).

### 9.4 How to read the logs

For the main CSV outputs produced by the v1 scripts:

- `backtest_signals.csv` (from `backtest_signals.py`):
  - Typical columns:
    - `ts`: Timestamp for the bar.
    - `ret_1h`: Realized 1h log return.
    - `p_up`: Direction model’s P(ret_1h > 0).
    - `ret_pred`: Regression model’s predicted 1h log return.
    - `signal_ensemble`: 1 if ensemble conditions are met, else 0.
    - `signal_dir_only`: 1 if direction-only P(up) >= 0.5, else 0.
    - `ret_net_ens`, `ret_net_dir`: Net (after-cost) log return per bar for ensemble and direction-only strategies.
    - `equity_ens`, `equity_dir`: Cumulative equity (exp of cumulative log return) per bar.
  - Simple checks:
    - Plot `equity_ens` and `equity_dir` over time to compare strategies.
    - Check that `ret_net_ens` is 0 on bars with `signal_ensemble = 0` (flat regime, apart from entry/exit costs).

- `paper_trade.csv` (from `paper_trade_loop.py`):
  - Typical columns:
    - `ts`: Timestamp for each bar in the simulated period.
    - `ret_1h`: Realized 1h log return.
    - `p_up`: Direction model’s predicted P(up).
    - `ret_pred`: Regression model’s predicted 1h log return.
    - `signal_ensemble`: Ensemble signal (0/1) for that bar.
    - `position`: Actual position held during the bar (0 = flat, 1 = long); matches `signal_ensemble` for that bar.
    - `ret_net`: Net log return for that bar, including entry/exit costs when position changes.
    - `equity`: Cumulative equity (starting from 1.0) computed as `exp(cumulative log net returns)`.
  - Simple checks:
    - Confirm that `position` only changes when `signal_ensemble` flips between 0 and 1.
    - Sum of `ret_net` over the period should equal `ln(equity[-1])`.
    - Plot `equity` over time to visualize the paper-trade equity curve and compare it to the backtest `equity_ens`.

- `paper_trade_realtime.csv` (from `run_signal_realtime.py`):
  - Columns:
    - `ts`: Timestamp of the bar for which the signal was generated.
    - `p_up`: Direction model’s P(up) at that bar.
    - `ret_pred`: Regression model’s predicted 1h log return.
    - `signal_ensemble`: Ensemble signal (0/1) based on current thresholds.
    - `signal_dir_only`: Direction-only signal (0/1, based on P(up) >= 0.5).
    - `created_at`: UTC timestamp when the signal row was logged.
    - `notes`: Freeform notes (typically empty in v1).
  - Simple checks:
    - Verify that `ts` is strictly increasing (at most one row per bar).
    - If the script is run hourly, ensure there are no duplicate `ts` values; the script should log a “skipping append” message if `ts` is unchanged.

---

## 10. Known Limitations & Next Steps

### Known Limitations

- **Transaction costs & slippage**: Only a simple per-trade bps model is applied; real-world costs depend on venue, liquidity, order type, and position sizing.
- **Only long positions**: No explicit shorting or neutral regimes beyond "no position".
- **Hyperparameters**: Models use hand-chosen hyperparameters, not the result of systematic tuning.
- **Futures features**: Futures-related features (e.g., `open_interest`, `funding_rate`) currently show low importance in feature importance analyses.
- **Single-horizon label**: Focus is only on 1h returns; other horizons are not explored.

### Next Steps

- Incorporate realistic transaction cost and slippage assumptions into evaluation.
- Extend threshold search to incorporate drawdown/volatility constraints, not just cumulative return.
- Confirm availability of OKLink API key for higher-frequency on-chain metrics.
- Revisit the futures feature engineering to try to extract more signal.
- Expand walk-forward evaluation (more folds, different window sizes, rolling refits).
- Add monitoring/alerting for live deployment (e.g., hit-rate drift, return distribution shifts).

### Phase 1 External Data Ingestion

- Macro loaders live under `data/ingestors/` (`fred_macro`, `alpha_vantage_macro`). Run them after exporting `FRED_API_KEY` and `ALPHA_VANTAGE_API_KEY`. Raw parquet lands in `data/raw/macro/...`.
- On-chain scaffolding uses `data.ingestors.blockchain_onchain`, writing tidy rows to `data/raw/onchain/...` with hourly processing handled by `data.processed.compute_onchain_features`.
- Funding aggregation is managed by `data.processed.compute_funding_features`. Use `--fetch` to hydrate `data/raw/funding/binance/...` before the hourly aggregation if live Binance access is available.
- Each processing script publishes both `data/processed/*/hourly_features.parquet` and an `artifacts/monitoring/*_summary.json` with row counts, missing ratios, and latest timestamps for quick validation.

---

## 11. Integrity & Leakage Checklist

- **No forward-looking targets as features**:
  - `ret_fwd_3h` is explicitly excluded from `X` in `make_features_and_target`.
  - Any future-looking columns must be addressed at the SQL/curation layer; Python preprocessing only uses non-target, non-forward features.
- **Chronological splits**:
  - All train/val/test splits (including walk-forward windows) respect time ordering.
- **Scaling**:
  - For the main datasets, `StandardScaler` is fit on train only and applied to validation/test.
  - Walk-forward evaluation trains models using only pre-window data, mimicking deployment.
- **Evaluation separation**:
  - Validation and test sets are never used for model fitting.
  - Walk-forward windows are strictly later than the data used to train the models evaluated on them.

This document describes **experiment version v1** for the 2024-10-01 to 2025-12-10 BTCUSDT 1h dataset. Future experiment versions (e.g., `experiment_2024-10_to-2025-12_v2.md`) should document any changes to data, features, models, thresholds, or evaluation methodology.

---

## 12. Optuna Hyperparameter Search Workflow

- **Script**: `src/scripts/search_xgb_optuna.py` centralizes Optuna tuning for the 1h regression (`ret_1h`) and direction models built from the curated dataset.
- **Key options**:
  - `--mode {reg|dir}` toggles between regression RMSE minimization and direction logloss minimization.
  - `--n-trials` (default `50`) and `--timeout` control the Optuna budget; use higher budgets for production sweeps.
  - `--storage` and `--study-name` enable SQLite/Postgres-backed studies so experiments can resume or aggregate results.
  - `--output-dir` captures artifacts: `best_model.json`, `best_summary.json`, and a legacy `summary.json` mirror for downstream tooling.
- **Workflow**:
  1. Loads features via `prepare_data_for_signals`, sorts chronologically, and applies the standard 70/15/15 split with a `StandardScaler` fit strictly on the training slice.
  2. Runs an Optuna study that samples depth, learning rate, subsampling, regularization, and (for direction) `scale_pos_weight`. Each trial trains on the train slice with early stopping on validation.
  3. Retrains the winning configuration on train+validation, scores on the held-out test slice, and persists metrics alongside model weights.
- **Example commands** (light 10-trial sweeps; increase `--n-trials` for thorough searches):

```bash
python -m src.scripts.search_xgb_optuna \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --mode reg \
  --n-trials 10 \
  --output-dir artifacts/analysis/optuna_reg_v1

python -m src.scripts.search_xgb_optuna \
  --dataset-path artifacts/datasets/btc_features_1h_splits.npz \
  --mode dir \
  --n-trials 10 \
  --output-dir artifacts/analysis/optuna_dir_v1
```

- **Outputs**: Each run prints the best trial summary and writes model weights plus validation/test metrics to the chosen output directory. Review `best_summary.json` to compare Optuna-tuned configurations against baseline hyperparameters before promoting them to backtests, realtime scripts, or artifact storage.

### Optuna v1 threshold search (Dec-2025 refresh)

- **Models used**: `artifacts/models/xgb_ret1h_optuna` + `artifacts/models/xgb_dir1h_optuna`, trained with the Optuna-selected hyperparameters captured in their respective `best_params.json` files and retrained summaries.
- **Validation sweep**: `python -m src.scripts.search_ensemble_thresholds --dataset-path artifacts/datasets/btc_features_1h_splits.npz --reg-model-dir artifacts/models/xgb_ret1h_optuna --dir-model-dir artifacts/models/xgb_dir1h_optuna --p-up-min-grid 0.40 0.45 0.50 0.55 --ret-min-grid -0.001 0.000 0.001 0.002 --fee-bps 2.0 --slippage-bps 1.0 --output-dir artifacts/analysis/threshold_search_optuna_v1`
  - Best combo on validation (net of 2 bps fee + 1 bps slippage): `p_up_min = 0.50`, `ret_min = -0.001`, delivering `n_trades = 810`, hit rate `0.528`, cum log return `0.425`, max drawdown `-0.047`.
- **Test backtest (net)**: `python -m src.scripts.backtest_signals --dataset-path artifacts/datasets/btc_features_1h_splits.npz --reg-model-dir artifacts/models/xgb_ret1h_optuna --dir-model-dir artifacts/models/xgb_dir1h_optuna --p-up-min 0.5 --ret-min -0.001 --fee-bps 2.0 --slippage-bps 1.0 --use-test-split --output-dir artifacts/analysis/backtest_signals_v1_optuna_thresholded`
  - Ensemble net metrics: `n_trades = 1,115`, hit rate `0.563`, avg ret/trade `0.00099`, cumulative log return `1.1011`, max drawdown `-0.079`. The signal collapses to the direction-only baseline under this threshold pair, matching its net results and improving over the earlier Optuna default thresholds (w/ `p_up_min = 0.45`, `ret_min = 0.0`) which posted `0.8977` net log return.
- **Artifacts**: threshold search outputs stored under `artifacts/analysis/threshold_search_optuna_v1/` (best_config + summary) and the aligned backtest log in `artifacts/analysis/backtest_signals_v1_optuna_thresholded/`.

### Optuna profile toggle (tooling integration)

- `src/scripts/run_signal_realtime.py`, `run_signal_realtime_from_binance.py`, `backtest_signals.py`, and `paper_trade_loop.py` now expose `--use-optuna-profile` to opt into the tuned 1h artifacts.
- When the flag is set, default arguments pointing at the v1 baseline models/thresholds are swapped for:
  - Models: `artifacts/models/xgb_ret1h_optuna` and `artifacts/models/xgb_dir1h_optuna`
  - Ensemble thresholds: `p_up_min = 0.50`, `ret_min = -0.001`
- Explicit CLI overrides (custom model dirs or thresholds) still win, so pass new values alongside the flag only when deviating from the baked-in Optuna pair.

### On-chain feature integration (roadmap phase kickoff)

- `src/data/onchain_loader.py` pulls BTC on-chain metrics (active addresses, transaction count, hash rate, market cap) through a configurable REST endpoint with retry/backoff, and provides `load_onchain_cached(path)` for CSV-based runs when credentials are unavailable.
- `src/scripts/build_training_dataset_multi_horizon.py` now accepts:
  - `--features-path` to bypass BigQuery with a local curated CSV containing `ts` + OHLC features.
  - `--onchain-path` to merge cached hourly metrics.
  - `--fetch-onchain` / `--onchain-interval` to source live data before falling back to cache.
- On-chain columns align on 1h timestamps and are forward/back-filled so the NPZ `feature_names` include them. Example commands:
  - Cached merge: `python -m src.scripts.build_training_dataset_multi_horizon --features-path data/mock_features_1h.csv --onchain-path data/onchain_btc_hourly.csv --output-path artifacts/datasets/btc_features_multi_horizon_onchain.npz`
  - Live fetch (requires `ONCHAIN_API_BASE_URL` + optional `ONCHAIN_API_KEY` env vars): `python -m src.scripts.build_training_dataset_multi_horizon --fetch-onchain --onchain-interval 1h --output-dir artifacts/datasets`
- Store cached data under `data/` for reproducibility; the resulting NPZ (`btc_features_multi_horizon_splits.npz`) now surfaces the on-chain series for downstream modeling and Optuna sweeps.

## 13. LSTM Direction Model v2 (Preview)

### Baseline checkpoint (manual config)

- **Training pipeline**: `src/scripts/train_lstm_dir1h.py` assembles 24-step sequences from `artifacts/datasets/btc_features_1h_direction_splits.npz`, imputes missing features with training means, and trains `LSTMDirectionClassifier` (CPU fallback for these runs).
- **Manual hyperparameters**: hidden size `128`, `2` stacked layers, dropout `0.2`, Adam (`lr=1e-3`, `batch_size=128`), patience `5`, seed `42`.
- **Baseline metrics (2025-12-12)**:
  - Train: accuracy `0.531`, precision `0.554`, recall `0.240`, F1 `0.335`, AUC `0.542`, loss `0.690`.
  - Validation: accuracy `0.522`, precision `0.563`, recall `0.097`, F1 `0.165`, AUC `0.527`, loss `0.694`.
  - Test: accuracy `0.529`, precision `0.508`, recall `0.357`, F1 `0.419`, AUC `0.529`, loss `0.692`.
- **Artifacts**: `artifacts/models/lstm_dir1h_v2/` (model.pt, scaler.joblib, summary.json).

### Optuna sweep (2025-12-12)

- **Search configuration**: `src/scripts/search_lstm_optuna.py` ran a 10-trial TPE study (median pruner) over hidden size, layers, dropout, norm type, learning rate, batch size, epochs, patience, and weight decay.
- **Best trial (val log-loss = 0.6914)**: hidden size `192`, `1` layer, dropout `0.033`, layer norm, `lr = 3.29e-4`, batch size `96`, epochs `6`, patience `8`, weight decay `1.08e-5`, delivering validation AUC `0.587` and test AUC `0.601` during the study run.
- **Retrained checkpoint**: `artifacts/models/lstm_dir1h_optuna/` (model.pt, scaler.joblib, summary.json, best_params.json) reproduces those hyperparameters; final metrics — train AUC `0.587`, val AUC `0.567`, test AUC `0.599` with test F1 `0.471`.

Backtest comparison on the 1h test split (`p_up_min = 0.45`, `ret_min = 0.0`, fees `2 bps`, slippage `1 bps`):

| Model + Direction Signal         | Strategy | Trades | Hit Rate | Cum Log Return (gross) | Cum Log Return (net) |
|----------------------------------|----------|-------:|---------:|-----------------------:|---------------------:|
| LSTM dir v2 (manual) + XGB ret   | Ensemble | 1,624  | 0.478    | -0.2739                | -0.7611              |
| LSTM dir v2 (manual)             | Dir-only |   539  | 0.508    |  0.1108                | -0.0509              |
| LSTM dir v2 Optuna + XGB ret     | Ensemble | 1,117  | 0.531    |  0.5899                |  0.2548              |
| LSTM dir v2 Optuna               | Dir-only |   573  | 0.551    |  0.5242                |  0.3523              |
| XGB dir Optuna + XGB ret         | Ensemble | 1,225  | 0.573    |  1.2652                |  0.8977              |
| XGB dir Optuna                   | Dir-only | 1,115  | 0.600    |  1.4356                |  1.1011              |

- **Takeaways**: the tuned LSTM materially improves on the untuned checkpoint (gross log return swings from `-0.274` to `+0.590` and net from `-0.761` to `+0.255`), yet still trails the XGBoost classifier, especially on trade selection and cumulative return. Validation recall remains low; future work should explore longer training schedules, per-window normalization, GRU/Transformer baselines, and joint threshold tuning tailored to neural outputs.
